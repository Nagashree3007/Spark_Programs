{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@Author: Nagashree C R<br>\n",
    "@Date: 2024-09-07<br>\n",
    "@Last Modified by: Nagashree C R<br>\n",
    "@Last Modified: 2024-09-07<br>\n",
    "@Title : Program to perform Read and write operations using different file formats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Csv File and Write to Json Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+-------------------+----------+------+---------+----------+----------------------------+---------+---------------------+\n",
      "|Active|Confirmed|Country/Region     |Date      |Deaths|Lat      |Long      |Province/State              |Recovered|WHO Region           |\n",
      "+------+---------+-------------------+----------+------+---------+----------+----------------------------+---------+---------------------+\n",
      "|0     |0        |Afghanistan        |2020-01-22|0     |33.93911 |67.709953 |NULL                        |0        |Eastern Mediterranean|\n",
      "|0     |0        |Albania            |2020-01-22|0     |41.1533  |20.1683   |NULL                        |0        |Europe               |\n",
      "|0     |0        |Algeria            |2020-01-22|0     |28.0339  |1.6596    |NULL                        |0        |Africa               |\n",
      "|0     |0        |Andorra            |2020-01-22|0     |42.5063  |1.5218    |NULL                        |0        |Europe               |\n",
      "|0     |0        |Angola             |2020-01-22|0     |-11.2027 |17.8739   |NULL                        |0        |Africa               |\n",
      "|0     |0        |Antigua and Barbuda|2020-01-22|0     |17.0608  |-61.7964  |NULL                        |0        |Americas             |\n",
      "|0     |0        |Argentina          |2020-01-22|0     |-38.4161 |-63.6167  |NULL                        |0        |Americas             |\n",
      "|0     |0        |Armenia            |2020-01-22|0     |40.0691  |45.0382   |NULL                        |0        |Europe               |\n",
      "|0     |0        |Australia          |2020-01-22|0     |-35.4735 |149.0124  |Australian Capital Territory|0        |Western Pacific      |\n",
      "|0     |0        |Australia          |2020-01-22|0     |-33.8688 |151.2093  |New South Wales             |0        |Western Pacific      |\n",
      "|0     |0        |Australia          |2020-01-22|0     |-12.4634 |130.8456  |Northern Territory          |0        |Western Pacific      |\n",
      "|0     |0        |Australia          |2020-01-22|0     |-27.4698 |153.0251  |Queensland                  |0        |Western Pacific      |\n",
      "|0     |0        |Australia          |2020-01-22|0     |-34.9285 |138.6007  |South Australia             |0        |Western Pacific      |\n",
      "|0     |0        |Australia          |2020-01-22|0     |-42.8821 |147.3272  |Tasmania                    |0        |Western Pacific      |\n",
      "|0     |0        |Australia          |2020-01-22|0     |-37.8136 |144.9631  |Victoria                    |0        |Western Pacific      |\n",
      "|0     |0        |Australia          |2020-01-22|0     |-31.9505 |115.8605  |Western Australia           |0        |Western Pacific      |\n",
      "|0     |0        |Austria            |2020-01-22|0     |47.5162  |14.5501   |NULL                        |0        |Europe               |\n",
      "|0     |0        |Azerbaijan         |2020-01-22|0     |40.1431  |47.5769   |NULL                        |0        |Europe               |\n",
      "|0     |0        |Bahamas            |2020-01-22|0     |25.025885|-78.035889|NULL                        |0        |Americas             |\n",
      "|0     |0        |Bahrain            |2020-01-22|0     |26.0275  |50.55     |NULL                        |0        |Eastern Mediterranean|\n",
      "+------+---------+-------------------+----------+------+---------+----------+----------------------------+---------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ReadWriteCSVtoJSON\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read CSV file\n",
    "csv_df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(r\"C:\\Users\\ASHAY\\Downloads\\covid-kaggle-dataset\\covid_19_clean_complete.csv\")\n",
    "\n",
    "# Write DataFrame to JSON file\n",
    "csv_df.write \\\n",
    "    .format(\"json\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(r\"C:/Users/ASHAY/OneDrive/Desktop/bridgelabz/spark/output.json\")\n",
    "\n",
    "# Read the JSON file back into a DataFrame\n",
    "json_df = spark.read \\\n",
    "    .format(\"json\") \\\n",
    "    .load(r\"C:/Users/ASHAY/OneDrive/Desktop/bridgelabz/spark/output.json\")\n",
    "\n",
    "# Show the contents of the JSON file\n",
    "json_df.show(truncate=False)  # `truncate=False` to avoid truncating long column values\n",
    "\n",
    "# Stop SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checking files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CovidProblems_Solution.sql\n",
      "file.py\n",
      "large_data.json\n",
      "output.json\n",
      "p1_deathpercentage.py\n",
      "p2_infectedpeople.py\n",
      "p3_highest_infection_rate.py\n",
      "p4_highest_death.py\n",
      "p5_averagedeaths.py\n",
      "p6_population.py\n",
      "p7_wordcount.py\n",
      "p8_read_write.ipynb\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Specify the directory path\n",
    "directory_path = r\"C:/Users/ASHAY/OneDrive/Desktop/bridgelabz/spark\"\n",
    "\n",
    "# List all files in the directory\n",
    "files = os.listdir(directory_path)\n",
    "\n",
    "# Print the list of files\n",
    "for file in files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read json write CSv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------------+----------+------+------------------+--------+--------------+---------+---------------+\n",
      "|Active|Confirmed|Country/Region|Date      |Deaths|Lat               |Long    |Province/State|Recovered|WHO Region     |\n",
      "+------+---------+--------------+----------+------+------------------+--------+--------------+---------+---------------+\n",
      "|0     |254      |China         |2020-04-26|2     |23.8298           |108.7881|Guangxi       |252      |Western Pacific|\n",
      "|0     |147      |China         |2020-04-26|2     |26.8154           |106.8748|Guizhou       |145      |Western Pacific|\n",
      "|0     |168      |China         |2020-04-26|6     |19.1959           |109.7453|Hainan        |162      |Western Pacific|\n",
      "|322   |328      |China         |2020-04-26|6     |39.549            |116.1306|Hebei         |0        |Western Pacific|\n",
      "|367   |936      |China         |2020-04-26|13    |47.861999999999995|127.7615|Heilongjiang  |556      |Western Pacific|\n",
      "|1254  |1276     |China         |2020-04-26|22    |37.8957           |114.9042|Henan         |0        |Western Pacific|\n",
      "|261   |1037     |China         |2020-04-26|4     |22.3              |114.2   |Hong Kong     |772      |Western Pacific|\n",
      "|0     |68128    |China         |2020-04-26|4512  |30.9756           |112.2707|Hubei         |63616    |Western Pacific|\n",
      "|0     |1019     |China         |2020-04-26|4     |27.6104           |111.7088|Hunan         |1015     |Western Pacific|\n",
      "|68    |198      |China         |2020-04-26|1     |44.0935           |113.9448|Inner Mongolia|129      |Western Pacific|\n",
      "|7     |653      |China         |2020-04-26|0     |32.9711           |119.455 |Jiangsu       |646      |Western Pacific|\n",
      "|0     |937      |China         |2020-04-26|1     |27.614            |115.7221|Jiangxi       |936      |Western Pacific|\n",
      "|10    |110      |China         |2020-04-26|1     |43.6661           |126.1923|Jilin         |99       |Western Pacific|\n",
      "|1     |146      |China         |2020-04-26|2     |41.2956           |122.6085|Liaoning      |143      |Western Pacific|\n",
      "|14    |45       |China         |2020-04-26|0     |22.1667           |113.55  |Macau         |31       |Western Pacific|\n",
      "|0     |75       |China         |2020-04-26|0     |37.2692           |106.1655|Ningxia       |75       |Western Pacific|\n",
      "|0     |18       |China         |2020-04-26|0     |35.7452           |95.9956 |Qinghai       |18       |Western Pacific|\n",
      "|30    |286      |China         |2020-04-26|3     |35.1917           |108.8701|Shaanxi       |253      |Western Pacific|\n",
      "|12    |787      |China         |2020-04-26|7     |36.3427           |118.1498|Shandong      |768      |Western Pacific|\n",
      "|67    |642      |China         |2020-04-26|7     |31.201999999999998|121.4491|Shanghai      |568      |Western Pacific|\n",
      "+------+---------+--------------+----------+------+------------------+--------+--------------+---------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ReadWriteJSONtoCSV\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read JSON file\n",
    "json_df = spark.read \\\n",
    "    .format(\"json\") \\\n",
    "    .load(r\"C:/Users/ASHAY/OneDrive/Desktop/bridgelabz/spark/output.json\")\n",
    "\n",
    "# Write DataFrame to CSV file\n",
    "csv_output_path = r\"C:/Users/ASHAY/OneDrive/Desktop/bridgelabz/spark/output.csv\"\n",
    "json_df.write \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(csv_output_path)\n",
    "\n",
    "# Read the CSV file back into a DataFrame\n",
    "csv_df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(csv_output_path)\n",
    "\n",
    "# Show the contents of the CSV file\n",
    "csv_df.show(truncate=False)  # `truncate=False` to avoid truncating long column values\n",
    "\n",
    "# Stop SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read json File and Write to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------------+----------+------+------------------+--------+--------------+---------+---------------+\n",
      "|Active|Confirmed|Country/Region|Date      |Deaths|Lat               |Long    |Province/State|Recovered|WHO Region     |\n",
      "+------+---------+--------------+----------+------+------------------+--------+--------------+---------+---------------+\n",
      "|0     |254      |China         |2020-04-26|2     |23.8298           |108.7881|Guangxi       |252      |Western Pacific|\n",
      "|0     |147      |China         |2020-04-26|2     |26.8154           |106.8748|Guizhou       |145      |Western Pacific|\n",
      "|0     |168      |China         |2020-04-26|6     |19.1959           |109.7453|Hainan        |162      |Western Pacific|\n",
      "|322   |328      |China         |2020-04-26|6     |39.549            |116.1306|Hebei         |0        |Western Pacific|\n",
      "|367   |936      |China         |2020-04-26|13    |47.861999999999995|127.7615|Heilongjiang  |556      |Western Pacific|\n",
      "|1254  |1276     |China         |2020-04-26|22    |37.8957           |114.9042|Henan         |0        |Western Pacific|\n",
      "|261   |1037     |China         |2020-04-26|4     |22.3              |114.2   |Hong Kong     |772      |Western Pacific|\n",
      "|0     |68128    |China         |2020-04-26|4512  |30.9756           |112.2707|Hubei         |63616    |Western Pacific|\n",
      "|0     |1019     |China         |2020-04-26|4     |27.6104           |111.7088|Hunan         |1015     |Western Pacific|\n",
      "|68    |198      |China         |2020-04-26|1     |44.0935           |113.9448|Inner Mongolia|129      |Western Pacific|\n",
      "|7     |653      |China         |2020-04-26|0     |32.9711           |119.455 |Jiangsu       |646      |Western Pacific|\n",
      "|0     |937      |China         |2020-04-26|1     |27.614            |115.7221|Jiangxi       |936      |Western Pacific|\n",
      "|10    |110      |China         |2020-04-26|1     |43.6661           |126.1923|Jilin         |99       |Western Pacific|\n",
      "|1     |146      |China         |2020-04-26|2     |41.2956           |122.6085|Liaoning      |143      |Western Pacific|\n",
      "|14    |45       |China         |2020-04-26|0     |22.1667           |113.55  |Macau         |31       |Western Pacific|\n",
      "|0     |75       |China         |2020-04-26|0     |37.2692           |106.1655|Ningxia       |75       |Western Pacific|\n",
      "|0     |18       |China         |2020-04-26|0     |35.7452           |95.9956 |Qinghai       |18       |Western Pacific|\n",
      "|30    |286      |China         |2020-04-26|3     |35.1917           |108.8701|Shaanxi       |253      |Western Pacific|\n",
      "|12    |787      |China         |2020-04-26|7     |36.3427           |118.1498|Shandong      |768      |Western Pacific|\n",
      "|67    |642      |China         |2020-04-26|7     |31.201999999999998|121.4491|Shanghai      |568      |Western Pacific|\n",
      "+------+---------+--------------+----------+------+------------------+--------+--------------+---------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ReadJSONtoParquet\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read JSON file\n",
    "json_df = spark.read \\\n",
    "    .format(\"json\") \\\n",
    "    .load(r\"C:/Users/ASHAY/OneDrive/Desktop/bridgelabz/spark/output.json\")\n",
    "\n",
    "# Write DataFrame to Parquet file\n",
    "parquet_output_path = r\"C:/Users/ASHAY/OneDrive/Desktop/bridgelabz/spark/output.parquet\"\n",
    "json_df.write \\\n",
    "    .format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(parquet_output_path)\n",
    "\n",
    "# Read the Parquet file back into a DataFrame (for verification or further processing)\n",
    "parquet_df = spark.read \\\n",
    "    .format(\"parquet\") \\\n",
    "    .load(parquet_output_path)\n",
    "\n",
    "# Show the contents of the Parquet file\n",
    "parquet_df.show(truncate=False)  # `truncate=False` to avoid truncating long column values\n",
    "\n",
    "# Stop SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Parquet File and Write to Avro Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------------+----------+------+------------------+--------+--------------+---------+---------------+\n",
      "|Active|Confirmed|Country/Region|Date      |Deaths|Lat               |Long    |Province/State|Recovered|WHO Region     |\n",
      "+------+---------+--------------+----------+------+------------------+--------+--------------+---------+---------------+\n",
      "|0     |254      |China         |2020-04-26|2     |23.8298           |108.7881|Guangxi       |252      |Western Pacific|\n",
      "|0     |147      |China         |2020-04-26|2     |26.8154           |106.8748|Guizhou       |145      |Western Pacific|\n",
      "|0     |168      |China         |2020-04-26|6     |19.1959           |109.7453|Hainan        |162      |Western Pacific|\n",
      "|322   |328      |China         |2020-04-26|6     |39.549            |116.1306|Hebei         |0        |Western Pacific|\n",
      "|367   |936      |China         |2020-04-26|13    |47.861999999999995|127.7615|Heilongjiang  |556      |Western Pacific|\n",
      "|1254  |1276     |China         |2020-04-26|22    |37.8957           |114.9042|Henan         |0        |Western Pacific|\n",
      "|261   |1037     |China         |2020-04-26|4     |22.3              |114.2   |Hong Kong     |772      |Western Pacific|\n",
      "|0     |68128    |China         |2020-04-26|4512  |30.9756           |112.2707|Hubei         |63616    |Western Pacific|\n",
      "|0     |1019     |China         |2020-04-26|4     |27.6104           |111.7088|Hunan         |1015     |Western Pacific|\n",
      "|68    |198      |China         |2020-04-26|1     |44.0935           |113.9448|Inner Mongolia|129      |Western Pacific|\n",
      "|7     |653      |China         |2020-04-26|0     |32.9711           |119.455 |Jiangsu       |646      |Western Pacific|\n",
      "|0     |937      |China         |2020-04-26|1     |27.614            |115.7221|Jiangxi       |936      |Western Pacific|\n",
      "|10    |110      |China         |2020-04-26|1     |43.6661           |126.1923|Jilin         |99       |Western Pacific|\n",
      "|1     |146      |China         |2020-04-26|2     |41.2956           |122.6085|Liaoning      |143      |Western Pacific|\n",
      "|14    |45       |China         |2020-04-26|0     |22.1667           |113.55  |Macau         |31       |Western Pacific|\n",
      "|0     |75       |China         |2020-04-26|0     |37.2692           |106.1655|Ningxia       |75       |Western Pacific|\n",
      "|0     |18       |China         |2020-04-26|0     |35.7452           |95.9956 |Qinghai       |18       |Western Pacific|\n",
      "|30    |286      |China         |2020-04-26|3     |35.1917           |108.8701|Shaanxi       |253      |Western Pacific|\n",
      "|12    |787      |China         |2020-04-26|7     |36.3427           |118.1498|Shandong      |768      |Western Pacific|\n",
      "|67    |642      |China         |2020-04-26|7     |31.201999999999998|121.4491|Shanghai      |568      |Western Pacific|\n",
      "+------+---------+--------------+----------+------+------------------+--------+--------------+---------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ReadParquetToAvro\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read Parquet file\n",
    "parquet_df = spark.read \\\n",
    "    .format(\"parquet\") \\\n",
    "    .load(r\"C:/Users/ASHAY/OneDrive/Desktop/bridgelabz/spark/output.parquet\")\n",
    "\n",
    "# Write DataFrame to Avro file\n",
    "avro_output_path = r\"C:/Users/ASHAY/OneDrive/Desktop/bridgelabz/spark/output.avro\"\n",
    "parquet_df.write \\\n",
    "    .format(\"avro\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(avro_output_path)\n",
    "\n",
    "# Read the Avro file back into a DataFrame (for verification or further processing)\n",
    "avro_df = spark.read \\\n",
    "    .format(\"avro\") \\\n",
    "    .load(avro_output_path)\n",
    "\n",
    "# Show the contents of the Avro file\n",
    "avro_df.show(truncate=False)  # `truncate=False` to avoid truncating long column values\n",
    "\n",
    "# Stop SparkSession\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
